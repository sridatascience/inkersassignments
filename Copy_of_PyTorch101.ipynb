{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PyTorch101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridatascience/inkersassignments/blob/master/Copy_of_PyTorch101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kYM3ylzYo1Q"
      },
      "source": [
        "![PyTorch](https://devblogs.nvidia.com/wp-content/uploads/2017/04/pytorch-logo-dark.png)\n",
        "\n",
        "An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcVqSTrWY6oz"
      },
      "source": [
        "# Tensor - Pytorch's core data structure\n",
        "\n",
        "In Python we can create lists, lists of lists, lists of lists and so on. In NumPy there is a `numpy.ndarray` which represents `n`- dimensional array. In math there is a special name for the generalization of vectors and matrices to a higher dimensional space - a tensor\n",
        "\n",
        "Tensor is an entity with a defined number of dimensions called an order (rank). \n",
        "\n",
        "**Scalar** can be considered as a rank-0-tensor. \n",
        "\n",
        "**Vector** can be introduced as a rank-1-tensor. \n",
        "\n",
        "**Matrices** can be considered as a rank-2-tensor.\n",
        "\n",
        "# Tensor Basics\n",
        "\n",
        "Let's import the torch module first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-oFVL2tYYqp"
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGxuR5IEaFJa"
      },
      "source": [
        "## Tensor Creation\n",
        "Let's view examples of matrices and tensors generation\n",
        "\n",
        "2-dimensional (rank-2) tensor of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897JQc25aD3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbb85e1-2c67-42f4-fbe7-016571595bcd"
      },
      "source": [
        "torch.zeros(3, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pujmpEBhaPRA"
      },
      "source": [
        "Random rank-3 tensor:\n",
        "_read the print below and convince yourself how this is a rank-3-tensor and learn what those 2, 3, 4 values are there for_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBrznTNhaOL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e9c265-f447-4b4c-e219-e3cddda68f73"
      },
      "source": [
        "torch.rand(2, 3, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.8414, 0.4143, 0.2398, 0.6944],\n",
              "         [0.3890, 0.7576, 0.0895, 0.7653],\n",
              "         [0.5442, 0.0254, 0.9606, 0.7398]],\n",
              "\n",
              "        [[0.8623, 0.0939, 0.1415, 0.8343],\n",
              "         [0.1940, 0.6431, 0.3841, 0.7688],\n",
              "         [0.6092, 0.2694, 0.0289, 0.2324]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO4fIr15asdi"
      },
      "source": [
        "I am hoping you have noticed 4-elements in a row, 3 rows making one block and there are 2 blocks. \n",
        "\n",
        "Random rank-4-tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiCLvMGCaVZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4121a1a-c89f-470c-ba44-a47904113446"
      },
      "source": [
        "torch.rand(2, 2, 2, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.1377, 0.7108, 0.4193],\n",
              "          [0.9998, 0.6114, 0.7843]],\n",
              "\n",
              "         [[0.0247, 0.1080, 0.8187],\n",
              "          [0.3893, 0.9098, 0.8072]]],\n",
              "\n",
              "\n",
              "        [[[0.7617, 0.6715, 0.3997],\n",
              "          [0.0702, 0.1704, 0.4237]],\n",
              "\n",
              "         [[0.2376, 0.1319, 0.0523],\n",
              "          [0.4549, 0.9336, 0.2219]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOrKM2ifNGgc",
        "outputId": "0003088a-d6fd-49f3-b2d2-1a1236fb01fe"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF6VL68s3AMV"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "How many dimensions are there in a tensor defined as below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IbXcI3x3Ibt"
      },
      "source": [
        "x=torch.rand(1, 1, 1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaxU6pFVQuHd",
        "outputId": "33e0077d-2276-4833-a6ae-e2b3829f4ff4"
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_OwENLVPhQ1"
      },
      "source": [
        "4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgpqssRa9jF"
      },
      "source": [
        "**bold text**.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are many more ways to create tensor using some restrictions on values it should contatn - for the full reference, please follow the [official docs](https://pytorch.org/docs/stable/torch.html#creation-ops). \n",
        "\n",
        "\n",
        ".\n",
        "---\n",
        "\n",
        "\n",
        "# Python / NumPy / Pytorch interoperability\n",
        "\n",
        "You can create tensors from python as well as numpy arrays. You can also convert torch tensors to numpy arrays. So, the interoperability between torch and numpy is pretty good. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipgpeWPfa6gE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f68f7f0-a127-48c6-8e46-700462ee3f12"
      },
      "source": [
        "# Simple Python List\n",
        "python_list = [1, 2]\n",
        "\n",
        "# Create a numpy array from python list\n",
        "numpy_array = np.array(python_list)\n",
        "\n",
        "numpy_array1 = np.array([1,2])\n",
        "\n",
        "print('numpy_array', numpy_array)\n",
        "print('numpy_array1', numpy_array1)\n",
        "# numpy_array[0]= 10\n",
        "\n",
        "print('numpy_array', numpy_array)\n",
        "# print('np-array',np.array([1,2])\n",
        "\n",
        "# Create a torch Tensor from python list\n",
        "tensor_from_list = torch.tensor(python_list)\n",
        "\n",
        "# Create a torch Tensor from Numpy array\n",
        "tensor_from_array = torch.tensor(numpy_array)\n",
        "\n",
        "# Another way to create a torch Tensor from Numpy array (share same storage)\n",
        "tensor_from_array_v2 = torch.from_numpy(numpy_array)\n",
        "\n",
        "# Convert torch tensor to numpy array\n",
        "array_from_tensor = tensor_from_array.numpy()\n",
        "print(\"x\"*20)\n",
        "print('List:   ', python_list)\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_list)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)\n",
        "print('Array:  ', array_from_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numpy_array [1 2]\n",
            "numpy_array1 [1 2]\n",
            "numpy_array [1 2]\n",
            "xxxxxxxxxxxxxxxxxxxx\n",
            "List:    [1, 2]\n",
            "Array:   [1 2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Array:   [1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylqp0z6XQ82R",
        "outputId": "6c7e4e4a-5053-4959-e0f1-9dd66b28e5fd"
      },
      "source": [
        "torch.tensor(numpy_array).numpy()\n",
        "#torch.tensor([1,2])\n",
        "#torch.tensor(np.array([1,2]))\n",
        "#torch.from_numpy(np.array([1,2]))\n",
        "#torch.from_numpy(np.array([1,2])).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_x-86B8gqik"
      },
      "source": [
        "**Difference between** `torch.Tensor` **and** `torch.from_numpy`\n",
        "\n",
        "Pytorch aims to be an effective library for computations. What does it mean? It means that pytorch avoids memory copying if it can. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCKDGpygn_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3144896-8ad2-4d76-f802-2a6a68be6fea"
      },
      "source": [
        "numpy_array[0] = 10\n",
        "# still a doubt if it is just printing already computed value for 'tensor_from_array'  why not for 'tensor_from_array_v2'\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "# print('Tensor:', torch.tensor(numpy_array))\n",
        "print('Tensor: ', tensor_from_array_v2)  #a torch Tensor from Numpy array (share same storage)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array:   [10  2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([10,  2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CKtdNRM3SMp"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "Assume that we moved our complete (cats vs dogs) image dataset to numpy arrays. Then we use torch.from_numpy to convert these images to tensor. Then we apply a specific data augmentation strategy called \"CutOut\" which blocks a portion of the image directly on these tensors. What will happen to the accuracy of a model trained on this strategy compared to the one without this strategy? CutOut strategy is shown below: \n",
        "\n",
        "![CutOut](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSnSyN835AmtQPKQbPjDHX-FmshNilbtexX95cRGQPwl56QCGDn)\n",
        "\n",
        "## Question 3:\n",
        "Why do you think we are observing this behavior?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUyuS3MnV_Yl"
      },
      "source": [
        "Question 2 - Model will not train further\n",
        "Question 3 - because we are not cutting out a portion of the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZq7O-aihSOA"
      },
      "source": [
        "We have two different ways to create tensor from its NumPy counterpart - one copies memory and another one shares the same underlying storage. It works in the opposite way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNFOwV8EhPwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fbafa98-382c-4b02-c128-57dc1888a1a0"
      },
      "source": [
        "array_from_tensor = tensor_from_array.numpy()\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)\n",
        "\n",
        "tensor_from_array[0] = 11\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor:  tensor([1, 2])\n",
            "Array:  [1 2]\n",
            "Tensor:  tensor([11,  2])\n",
            "Array:  [11  2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM9ytbvKhtCw"
      },
      "source": [
        "## Data types\n",
        "\n",
        "The basic data type of all Deep Learning-related operations is float, but sometimes you may need something else. Pytorch support different number types for its tensors the same way NumPy does it - by specifying the data type on tensor creation or via casting. Ths full list of supported data types can be found [here](https://pytorch.org/docs/stable/tensors.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd6WkzJ4hpYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223246dc-5b4a-4527-bb37-20762d79dc54"
      },
      "source": [
        "tensor = torch.zeros(2, 2)\n",
        "print('Tensor with default type: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.float16)\n",
        "print('Tensor with 16-bit float: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.int16)\n",
        "print('Tensor with integers: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.bool)\n",
        "print('Tensor with boolean data: ', tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor with default type:  tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "Tensor with 16-bit float:  tensor([[0., 0.],\n",
            "        [0., 0.]], dtype=torch.float16)\n",
            "Tensor with integers:  tensor([[0, 0],\n",
            "        [0, 0]], dtype=torch.int16)\n",
            "Tensor with boolean data:  tensor([[False, False],\n",
            "        [False, False]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9F4Dkdr40TE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 4:\n",
        "We saw above that some times numpy and tensors share same storage and changing one changes the other. \n",
        "If we define a rank-2-tensor with ones (dtype of f16), and then convert it into a numpy data type using tensor.numpy() and store it in a variable called \"num\", and then we perform this operation `num = num * 0.5`, will the original tensor have 1.0s or 0.5s as its element values? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsEYj_J7tN81",
        "outputId": "19e23066-e513-44c8-d9d7-bb013e1aeaa5"
      },
      "source": [
        "num = torch.ones(2,2,dtype=torch.float16).numpy()\n",
        "print(num)\n",
        "num=num*0.5\n",
        "print(num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1.]\n",
            " [1. 1.]]\n",
            "[[0.5 0.5]\n",
            " [0.5 0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3hIuVILuWlt"
      },
      "source": [
        "0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQt8Mmm51OE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: \n",
        "If the operation `num = num*5` is changed to `num[:] = num*5` will the original tensor have 1.0s or 0.5s as its element values? \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwfSesDeuroX",
        "outputId": "89dc4b89-bb46-487f-f2fb-e0590dd23c7c"
      },
      "source": [
        "num = torch.ones(2,2,dtype=torch.float16).numpy()\n",
        "print(num)\n",
        "num[:]=num*5\n",
        "print(num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1.]\n",
            " [1. 1.]]\n",
            "[[5. 5.]\n",
            " [5. 5.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMOnqe2svN4m"
      },
      "source": [
        "5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzh8UB8KiVmb"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "Tensor provides access to its elements via the same `[]` operation as a regular python list or NumPy array. However, as you may recall from NumPy usage, the full power of math libraries is accessible only via vectorized operations, i.e. operations without explicit looping over all vector elements in python and using implicit optimized loops in C/C++/CUDA/Fortran/etc. available via special function calls. Pytorch employs the same paradigm and provides a wide range of vectorized operations. Let's take a look at some examples. \n",
        "\n",
        "Joining a list of tensors together with `torch.cat`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMaCDKPhiUAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6024746f-a7c3-4b8a-f5dc-0920736ddc5f"
      },
      "source": [
        "a = torch.zeros(3, 2)\n",
        "b = torch.ones(3, 2)\n",
        "print(torch.cat((a, b), dim=0))\n",
        "print(torch.cat((a, b), dim=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bj4aeE86zdH"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 6: \n",
        "Is the transpose of concatenated a & b tensor on dimension 1, same as the contatenated tensor of a & b on dimension 0? \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbhrfiPyHT-"
      },
      "source": [
        "No"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJLgKAK9yG_a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TXNne69j7LP"
      },
      "source": [
        "Indexing with another tenxor/array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-Fsi4jVd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713c52c6-7816-4ad6-aab2-edf9eee4a3b5"
      },
      "source": [
        "a = torch.arange(start=0, end=10)\n",
        "indices = np.arange(0, 10) > 5\n",
        "print(a)\n",
        "print(indices)\n",
        "print(a[indices])\n",
        "\n",
        "print(\"x\"*20)\n",
        "indices = torch.arange(start=0, end=10) %5\n",
        "print(indices)\n",
        "print(a[indices])\n",
        "b=a[indices[-5:]]\n",
        "print('b=',b)\n",
        "print(\"x\"*20)\n",
        "\n",
        "\n",
        "\n",
        "indices1 = torch.arange(start=1, end=11) %5\n",
        "print(indices1)\n",
        "b=a[indices1[-5:]]\n",
        "print('b= ',b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "[False False False False False False  True  True  True  True]\n",
            "tensor([6, 7, 8, 9])\n",
            "xxxxxxxxxxxxxxxxxxxx\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "b= tensor([0, 1, 2, 3, 4])\n",
            "xxxxxxxxxxxxxxxxxxxx\n",
            "tensor([1, 2, 3, 4, 0, 1, 2, 3, 4, 0])\n",
            "b=  tensor([1, 2, 3, 4, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4dnlu47WQu"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 7:\n",
        "\n",
        "`a` is defined as `torch.arange(start=0, end=10)`. We will create `b` using the two operations as below. In both cases do we get the same value?\n",
        "\n",
        "\n",
        "1.   indices variable created by the modulo operation on arange between 0 and 10. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "2.   indices variable created by the modulo operation on arange betwenn 1 and 11. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_y4_ccN6et"
      },
      "source": [
        "No"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEQeVxuDN6Ql"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ4ZCVsTk-KH"
      },
      "source": [
        "What should we do if we have, say, rank-2-tensor and want to select only some rows?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtRpotjkt1q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "89ba8b02-10d3-4c37-982a-3ef4b05be946"
      },
      "source": [
        "tensor = torch.rand((5, 3))\n",
        "rows = torch.tensor([0, 2])\n",
        "print(tensor)\n",
        "tensor[rows]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6657, 0.7041, 0.4038],\n",
            "        [0.1543, 0.4606, 0.1103],\n",
            "        [0.6704, 0.6282, 0.1762],\n",
            "        [0.7504, 0.6413, 0.5980],\n",
            "        [0.0452, 0.2649, 0.7961]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6657, 0.7041, 0.4038],\n",
              "        [0.6704, 0.6282, 0.1762]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_5nMKe32E-0",
        "outputId": "1ff4a6a3-e568-4a14-f62f-1c1c541fc98d"
      },
      "source": [
        "tensor = torch.rand((6,5))\n",
        "rows= torch.tensor([0,2,4])\n",
        "print(tensor)\n",
        "print(tensor[rows])\n",
        "torch.transpose(tensor[rows],0,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1784, 0.9534, 0.5673, 0.9545, 0.6381],\n",
            "        [0.6061, 0.5048, 0.4954, 0.6176, 0.4644],\n",
            "        [0.7596, 0.2594, 0.1323, 0.0272, 0.2814],\n",
            "        [0.5297, 0.0819, 0.1265, 0.1852, 0.7114],\n",
            "        [0.3284, 0.6695, 0.5403, 0.4938, 0.5924],\n",
            "        [0.3430, 0.9531, 0.8495, 0.5548, 0.5554]])\n",
            "tensor([[0.1784, 0.9534, 0.5673, 0.9545, 0.6381],\n",
            "        [0.7596, 0.2594, 0.1323, 0.0272, 0.2814],\n",
            "        [0.3284, 0.6695, 0.5403, 0.4938, 0.5924]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1784, 0.7596, 0.3284],\n",
              "        [0.9534, 0.2594, 0.6695],\n",
              "        [0.5673, 0.1323, 0.5403],\n",
              "        [0.9545, 0.0272, 0.4938],\n",
              "        [0.6381, 0.2814, 0.5924]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIJnr_N2_Qaf"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 8: \n",
        "\n",
        "Consider a tensor defined as `torch.rand((6, 5))`. Is the shape of the new tensor created by taking the 0th, 2nd and 4th row of the old tensor same as the shape of the a newer tensor created by taking the 0th, 2nd and 4th row of the old tensor after transposing it by operation `torch.transpose(tensor, 0, 1)` ?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jau44TY75zct"
      },
      "source": [
        "No\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naDzFMkslU0b"
      },
      "source": [
        "## Tensor Shapes\n",
        "\n",
        "Reshaping a tensor is a frequently used operation. We can change the shape of a tensor without the memory copying overhead. There are two methods for that: `reshape` and `view`. \n",
        "\n",
        "The difference is the following: \n",
        "\n",
        "\n",
        "*   view tries to return a tensor, and it shares the same memory with the original tensor. In case, if it cannot reuse the same memory due to [some reason](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view), it just fails. \n",
        "*   reshape always returns the tensor with the desired shape and tries to reuse the memory. If it cannot, it creates a copy\n",
        "\n",
        "Let's see with the help of an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HClDkqLLlMJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bca53d0-bbf9-4f59-825c-c2a70a1031e8"
      },
      "source": [
        "tensor = torch.rand(2, 3, 4)\n",
        "print('Pointer to data: ', tensor.data_ptr())\n",
        "print('Shape: ', tensor.shape)\n",
        "print('x'*20)\n",
        "reshaped = tensor.reshape(24)\n",
        "\n",
        "view = tensor.view(3, 2, 4)\n",
        "print('Reshaped tensor - pointer to data', reshaped.data_ptr())\n",
        "print('Reshaped tensor shape ', reshaped.shape)\n",
        "print('x'*20)\n",
        "print('Viewed tensor - pointer to data', view.data_ptr())\n",
        "print('Viewed tensor shape ', view.shape)\n",
        "print('x'*20)\n",
        "assert tensor.data_ptr() == view.data_ptr()\n",
        "\n",
        "assert np.all(np.equal(tensor.numpy().flat, reshaped.numpy().flat))\n",
        "\n",
        "print('Original stride: ', tensor.stride())\n",
        "print('Reshaped stride: ', reshaped.stride())\n",
        "print('Viewed stride: ', view.stride())\n",
        "print('x'*20)\n",
        "\n",
        "print(torch.rand(2, 3, 4).stride())\n",
        "x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
        "print(x.t())\n",
        "x.t().stride()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pointer to data:  106741632\n",
            "Shape:  torch.Size([2, 3, 4])\n",
            "xxxxxxxxxxxxxxxxxxxx\n",
            "Reshaped tensor - pointer to data 106741632\n",
            "Reshaped tensor shape  torch.Size([24])\n",
            "xxxxxxxxxxxxxxxxxxxx\n",
            "Viewed tensor - pointer to data 106741632\n",
            "Viewed tensor shape  torch.Size([3, 2, 4])\n",
            "xxxxxxxxxxxxxxxxxxxx\n",
            "Original stride:  (12, 4, 1)\n",
            "Reshaped stride:  (1,)\n",
            "Viewed stride:  (8, 4, 1)\n",
            "xxxxxxxxxxxxxxxxxxxx\n",
            "(12, 4, 1)\n",
            "tensor([[ 1.,  6.],\n",
            "        [ 2.,  7.],\n",
            "        [ 3.,  8.],\n",
            "        [ 4.,  9.],\n",
            "        [ 5., 10.]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07Kd9shARCBF",
        "outputId": "19792264-3b85-465a-e714-037e298c20bb"
      },
      "source": [
        "np.all([[True,True],[True,False]])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIN5jSppm4yC"
      },
      "source": [
        "The basic rule about reshaping the tensor is definitely that you cannot change the total number of elements in it, so the product of all tensor's dimensions should always be the same. It gives us the ability to avoid specifying one dimension when reshaping the tensor - Pytorch can calculate it for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3D19ERFmzOl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b49f6393-7a79-4da7-bb31-b5d8cd885e3a"
      },
      "source": [
        "print(tensor.reshape(3, 2, 4).shape)\n",
        "print(tensor.reshape(3, 2, -1).shape)\n",
        "print(tensor.reshape(3, -1, 4).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObgCQKUiETak"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 9:\n",
        "\n",
        "Consider a tensor `a` created with [1, 2, 3] and [1, 2, 3] of size (2, 3) is reshaped with operation `.reshape(-1, 2)`. Also consider a tensor `b` created with [[2, 1]] and of size (1, 2), later operated with `view(2, -1)` operation. \n",
        "\n",
        "If we do a dot product of a and b (using `torch.mm`) and perform the sum of all the elements (using `torch.sum`) what do we get? (enter int value without any decimal point in the quiz)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-n8pvY6idXE",
        "outputId": "f2260e50-c3d1-4fee-a2e8-d66c84700785"
      },
      "source": [
        "a = torch.tensor([[1,2,3],\n",
        "   [1,2,3]]).reshape(-1,2)\n",
        "a\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 1],\n",
              "        [2, 3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li60G6NOl_8J",
        "outputId": "3e408571-330e-4b82-b56e-8e4b87468f20"
      },
      "source": [
        "b = torch.tensor([[2,1]]).view(2,-1)\n",
        "b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2],\n",
              "        [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JiZrMo2mZAT",
        "outputId": "e2b5aa08-3db6-4d75-8308-e379262aa795"
      },
      "source": [
        "torch.sum(torch.mm(a,b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3jlfvyqm25G"
      },
      "source": [
        "tensor(18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX2Ry29hidHw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mza7QPg3ndeV"
      },
      "source": [
        "**Alternative ways to view tensors** - `expand` or `expand_as`.\n",
        "\n",
        "\n",
        "\n",
        "*   `expand` - requires the desired shape as an input\n",
        "*   `expand_as` - uses the shape of another tensor\n",
        "\n",
        "These operations \"repeat\" tensor's values along the specified axes without actually copying the data. \n",
        "\n",
        "As the documentation says, expand:\n",
        "\n",
        "\n",
        "> returns a new view of the self tensor with singleton dimensions expanded to a larger size. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. \n",
        "\n",
        "**Use case:**\n",
        "\n",
        "\n",
        "\n",
        "*   index multi-channel tensor with single-channel mask - imagine a color image with 3 channels (RGB) and binary mask for the area of interest on that image. We cannot index the image with this kind of mask directly since the dimensions are different, but we can use `expand_as` operation to create a view of the mask that has the same dimensions as the image we want to apply it to, but has not copied the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz33E-V7nPQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77df008c-62e4-4675-8292-8022d3dbfba8"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create a black image\n",
        "image = torch.zeros(size=(3, 256, 256), dtype=torch.int)\n",
        "# print(image)\n",
        "\n",
        "# Leave the borders and make the rest of the image Green\n",
        "image[1, 18:256 - 18, 18:256 - 18] = 255\n",
        "print(image)\n",
        "\n",
        "# Create a mask of the same size\n",
        "mask = torch.zeros(size=(256, 256), dtype=torch.bool)\n",
        "print(mask)\n",
        "\n",
        "# Assuming the green region in the original image is the Region of interest, change the mask to white for that area\n",
        "mask[18:256 - 18, 18:256 - 18] = 1\n",
        "\n",
        "# Create a view of the mask with the same dimensions as the original image\n",
        "mask_expanded = mask.expand_as(image)\n",
        "print(mask_expanded.shape)\n",
        "\n",
        "mask_np = mask_expanded.numpy().transpose(1, 2, 0) * 255\n",
        "print(mask_np)\n",
        "image_np = image.numpy().transpose(1, 2, 0)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[0, mask] += 128\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[mask_expanded] += 128\n",
        "image.clamp_(0, 255)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int32)\n",
            "tensor([[False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        ...,\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False]])\n",
            "torch.Size([3, 256, 256])\n",
            "[[[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]\n",
            "\n",
            " [[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]\n",
            "\n",
            " [[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]\n",
            "\n",
            " [[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]\n",
            "\n",
            " [[0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  ...\n",
            "  [0 0 0]\n",
            "  [0 0 0]\n",
            "  [0 0 0]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiXBx0k3ptOI"
      },
      "source": [
        "In the example above, one can also find a couple of useful tricks:\n",
        "\n",
        "\n",
        "*   `clamp` method and function is a Pytorch's analogue of NumPy's `clip` function\n",
        "*   many operations on tensors have in-place form, that does not return modified data, but change values in the tensor. The in-place version of the operation has trailing underscore according to Pytorch's naming convension - in the exmaple above it is `clamp_`\n",
        "*   tensors have the same indexing as Numpy's arrays - one can use `:` seperated range, negative indexes and so on.\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Images and their representations\n",
        "\n",
        "Now, let's discuss images, their representations and how different Python librarties work with them. \n",
        "\n",
        "Probably, the most well-known library for image loading and simple processing is [Pillow](https://pillow.readthedocs.io/en/stable/). \n",
        "\n",
        "However, many people in deep learning area stick with OpenCV for image loading and processing with some usage of another libraries when it is justified by performance/functionality. This is because OpenCV is in general much faster than the other libraries. Here you can find a couple of benchmarks: \n",
        "\n",
        "*   https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading\n",
        "*   https://github.com/albumentations-team/albumentations#benchmarking-results\n",
        "\n",
        "To sum up the benchmarks above, there are two most common image formats, PNG and JPEGs. If your data is in PNG format - use OpenCV to read it. If it is in JPEG - use libturbojpeg. For image processing, use OpenCV if possible. _We will be using PIL a lot along with these._\n",
        "\n",
        "As you will read the code from others, you may find out that some of them use Pillow/something else to read data. You should know, that color image representations in OpenCV and other libraries are different - OpenCV uses \"BGR\" channel order, while others use \"RGB\" one. \n",
        "\n",
        "To change \"BRG\" <-> \"RGB\" the only thing we need to do it to change channel order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYv4sZMmpndu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "e43855ea-df96-4f6d-8521-930e08257a28"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "\n",
        "bgr_image = cv2.imread('globeimage.jpg') \n",
        "#testing\n",
        "\n",
        "# remember to add your own image in case you run this block, if you want to use the same image, \n",
        "# download it from: https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRCA40ftnscVzfV8ft8e7vIzQXfXeZdtco8nknJrfCUW6INI40U\n",
        "rgb_image = bgr_image[..., ::-1]\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(bgr_image)\n",
        "ax[1].imshow(rgb_image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoi4TeNBGHle"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 10:\n",
        "\n",
        "Looking at the results above it can be said that the pixel values in the blue channels would be very small compared to red channel. True/False?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWffmV-eyyKY"
      },
      "source": [
        "False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ugFd57zNwa"
      },
      "source": [
        "# Autograd\n",
        "\n",
        "Pytorch supports automatic differentiation. The module which implements this is called **AutoGrad**. It calculates the gradients and keeps track in forward and backward passes. For primitive tensors, you need to enable or disable it using the `required_grad` flag. But, for advanced tensors, it is enabled by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMOp4aiou6JR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "e9a22acd-21b1-427d-c1c0-90302bc1c37a"
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad = True)\n",
        "print(a)\n",
        "result = a * 5\n",
        "print(result)\n",
        "\n",
        "# grad can be implicitly created only for scalar outputs\n",
        "# so let's calculate the sum here so that the output becomes a scalar and we can apply a backward pass\n",
        "mean_result = result.sum()\n",
        "print(mean_result)\n",
        "# calculate gradient\n",
        "mean_result.backward()\n",
        "# print gradient of a\n",
        "print(a.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1420, 0.0529, 0.6277, 0.6623, 0.4401],\n",
            "        [0.0893, 0.8400, 0.5712, 0.4038, 0.1173],\n",
            "        [0.3958, 0.8571, 0.1979, 0.5202, 0.4367]], requires_grad=True)\n",
            "tensor([[0.7102, 0.2647, 3.1385, 3.3117, 2.2004],\n",
            "        [0.4464, 4.2000, 2.8562, 2.0192, 0.5864],\n",
            "        [1.9788, 4.2853, 0.9895, 2.6009, 2.1835]], grad_fn=<MulBackward0>)\n",
            "tensor(31.7716, grad_fn=<SumBackward0>)\n",
            "tensor([[5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkyVdQmNPHix",
        "outputId": "015e2e69-bf54-4937-b220-d73b2205419f"
      },
      "source": [
        "b=torch.rand((4,5), requires_grad=True)\n",
        "print(b)\n",
        "result = b*15\n",
        "print(result)\n",
        "mean_result = result.sum()\n",
        "print(mean_result)\n",
        "# calculate gradient\n",
        "mean_result.backward()\n",
        "# print gradient of a\n",
        "print(b.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3877, 0.5639, 0.4215, 0.3233, 0.5905],\n",
            "        [0.8372, 0.4401, 0.6552, 0.7870, 0.6326],\n",
            "        [0.9920, 0.3999, 0.6073, 0.0935, 0.6155],\n",
            "        [0.4251, 0.2229, 0.3127, 0.5406, 0.8400]], requires_grad=True)\n",
            "tensor([[ 5.8156,  8.4591,  6.3220,  4.8493,  8.8581],\n",
            "        [12.5584,  6.6015,  9.8273, 11.8056,  9.4892],\n",
            "        [14.8804,  5.9983,  9.1088,  1.4018,  9.2325],\n",
            "        [ 6.3770,  3.3428,  4.6911,  8.1095, 12.5993]], grad_fn=<MulBackward0>)\n",
            "tensor(160.3274, grad_fn=<SumBackward0>)\n",
            "tensor([[15., 15., 15., 15., 15.],\n",
            "        [15., 15., 15., 15., 15.],\n",
            "        [15., 15., 15., 15., 15.],\n",
            "        [15., 15., 15., 15., 15.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4yn_NK8QhWA",
        "outputId": "5a186301-96ce-4709-d138-5c9bc4800b4e"
      },
      "source": [
        "x=torch.tensor(2.0, requires_grad=True)\n",
        "y=8*x**4+3*x**3+7*x**2+6*x+3\n",
        "y.backward()\n",
        "x.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(326.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym0Amk2IGfLx"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 11: \n",
        "\n",
        "Why the gradient of a is all 5s above?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PDgGq2R0k7I"
      },
      "source": [
        "As we see, Pytorch automagically calculated the gradient value for us. It looks to be the correct value - we multiplied an input by 5, so the gradient of this operation equals to 5.\n",
        "\n",
        "# Disabling Autograd for tensors\n",
        "\n",
        "We don't need to compute gradients for all the variables that are involved in the pipeline. The Pytorch API provides 2 ways to disable autograd.\n",
        "\n",
        "`detach` - returns a copy of the tensor with autograd disabled. This \n",
        "\n",
        "1.   copy is built on the same memory as the original tensor, so in-place size / stride / storage changes (such as resize_ / resizeas / set / transpose) modifications are not allowed.\n",
        "2.   torch.no_grad() - It is a context manager that allows you to guard a series of operations from autograd without creating new tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqVG9fQb0cLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd656a8-650c-4fdf-e05a-ab3c3e9248fb"
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "detached_a = a.detach()\n",
        "detached_result = detached_a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqpch2Be02J7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0a5ee8e8-81b8-46f0-c7fc-ac406657e928"
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    detached_result = a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcJfsIyD47cf",
        "outputId": "d278a8b2-26a2-4f04-d398-e8f152cef0bc"
      },
      "source": [
        "x=1e-6\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1e-06"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjh2rYOPJUAZ"
      },
      "source": [
        "# Custom Network\n",
        "\n",
        "A fully-connected ReLU network with one hidden layer and no biases, trained to predict y from x by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses PyTorch tensors to manually compute the forward pass, loss, and backward pass.\n",
        "\n",
        "A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.\n",
        "\n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nf5RaB104Vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2f42e9-ed44-4e52-853b-8b1466fb48a1"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "print(x)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "print(y)\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.mm(w1)\n",
        "    h_relu = h.clamp(min=0)\n",
        "    y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2*(y_pred - y)\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "    grad_h = grad_h_relu.clone()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.5358, -2.1213,  0.2257,  ...,  0.4464, -1.0751, -0.6524],\n",
            "        [ 0.5016, -0.5708,  0.5187,  ..., -1.4159, -1.0813, -0.8866],\n",
            "        [ 0.1323, -0.2142, -0.1180,  ...,  0.3175,  0.0579, -1.0399],\n",
            "        ...,\n",
            "        [-2.1776,  1.4148, -0.6765,  ..., -0.1035, -0.2440, -1.4680],\n",
            "        [ 0.3094, -0.6734,  1.6844,  ...,  0.2700,  0.0451,  0.7579],\n",
            "        [-1.2432, -0.1760, -0.9821,  ...,  0.6548,  1.7105, -0.1998]])\n",
            "tensor([[-1.3700e+00, -5.8635e-01,  3.7315e-01, -2.6689e+00, -1.0111e-02,\n",
            "          7.0651e-01, -1.6995e-01,  8.6909e-01,  1.1697e+00, -1.7667e+00],\n",
            "        [ 1.3134e+00,  9.0526e-03,  1.3033e+00,  2.8880e-01,  3.2253e-01,\n",
            "         -2.8124e+00,  2.6585e-01,  2.5602e-01,  4.9772e-01,  8.4817e-01],\n",
            "        [ 1.0244e+00, -9.1175e-01,  1.7089e+00,  8.6911e-01, -2.4813e-01,\n",
            "         -1.2078e+00,  2.3891e-01,  1.8243e+00,  7.8236e-01, -7.2419e-01],\n",
            "        [ 1.8982e-01,  4.9565e-01, -1.5803e+00, -1.6486e+00, -8.1425e-01,\n",
            "         -8.8998e-02, -1.4406e+00,  2.0159e-01, -6.4698e-02,  5.7053e-01],\n",
            "        [-6.5458e-01, -1.0796e+00, -9.9706e-01, -1.6537e+00, -1.1954e+00,\n",
            "          7.2037e-01, -8.7471e-02, -3.8804e-01, -1.0621e+00,  2.0681e-01],\n",
            "        [ 1.1760e+00,  9.5084e-01, -3.1131e-01, -1.1297e+00, -1.8064e+00,\n",
            "         -3.6984e-01,  1.1108e-01, -1.8929e-01,  4.4662e-01,  1.0562e+00],\n",
            "        [-4.3173e-02, -2.3396e-01,  1.9619e-01, -1.4557e+00, -5.2160e-02,\n",
            "         -3.5633e-01, -1.0762e+00,  1.1496e-01,  3.0910e-01, -2.7864e-02],\n",
            "        [ 4.0099e-01, -1.8884e+00, -1.9235e-01,  1.1249e-01, -4.3844e-01,\n",
            "         -3.0311e-01,  8.1290e-01,  3.8605e-01, -2.0673e+00, -6.0080e-01],\n",
            "        [-1.1169e+00, -6.1634e-01, -1.9034e+00, -8.8114e-02, -1.9945e-01,\n",
            "          1.0960e+00, -5.0559e-02, -8.3478e-01,  3.3660e-01, -5.5136e-01],\n",
            "        [ 6.8488e-01,  1.2810e+00, -1.2263e-01,  5.1111e-01,  1.4881e+00,\n",
            "          5.7949e-01, -6.4798e-01,  8.6543e-01, -8.8241e-01,  1.0259e+00],\n",
            "        [-5.3483e-01, -1.6882e+00,  1.6432e+00, -5.0524e-01, -1.9773e-01,\n",
            "         -1.2906e-01,  1.7933e+00,  1.6016e-01, -2.2875e+00,  1.5961e+00],\n",
            "        [-3.1773e-01, -1.8137e-01, -1.3908e-01, -2.9331e-01,  1.2700e+00,\n",
            "          1.0726e+00, -1.0907e+00, -8.1682e-01, -3.9943e-02,  7.0020e-02],\n",
            "        [-4.7325e-01,  5.5748e-01,  1.7336e+00,  9.3941e-01,  2.5697e-01,\n",
            "          1.9837e-01,  1.3323e+00, -8.8620e-02,  1.7988e-01, -7.0258e-01],\n",
            "        [-1.2106e+00, -9.6982e-01,  7.7627e-01,  5.3146e-01, -2.7662e-01,\n",
            "          1.7559e+00,  1.0811e-01,  5.8367e-01,  2.5384e-01, -5.2325e-01],\n",
            "        [-8.7700e-01, -6.4747e-01,  6.5002e-02, -1.4175e+00,  9.6692e-02,\n",
            "          9.9599e-01,  6.4131e-01,  9.6630e-01, -2.5021e-01, -1.2455e+00],\n",
            "        [ 2.4771e+00, -1.0038e+00, -2.9263e-01, -8.9648e-02,  7.2278e-01,\n",
            "          9.0253e-01,  2.1183e+00, -8.2295e-01,  4.2058e-01,  7.7815e-02],\n",
            "        [-1.8006e+00,  2.0721e-01,  1.2819e-01,  1.0324e+00, -1.5511e+00,\n",
            "          9.5808e-01,  1.4253e+00, -1.5529e+00,  7.8186e-01, -1.7099e+00],\n",
            "        [ 1.1339e+00, -3.3088e-01,  4.7912e-01, -1.1243e+00, -1.4631e+00,\n",
            "          1.6132e+00, -1.0153e+00, -1.3950e+00, -6.6098e-02, -5.4470e-01],\n",
            "        [-9.8982e-02,  6.0969e-01, -1.4479e-01,  1.1390e+00, -6.7037e-01,\n",
            "         -9.2106e-01,  1.1096e+00,  1.9121e-01,  6.7545e-01, -6.9577e-01],\n",
            "        [-4.6177e-01,  2.5139e-01,  2.3251e-01,  9.5321e-01,  4.9466e-02,\n",
            "          1.7367e+00, -6.9154e-01, -6.0782e-01, -9.0469e-01,  1.1002e+00],\n",
            "        [ 1.8696e+00, -1.7777e+00, -5.7012e-01, -7.0333e-01, -6.3183e-01,\n",
            "         -2.2273e-01, -2.1401e-01,  1.0637e+00,  1.6134e+00,  1.2913e+00],\n",
            "        [ 6.9505e-01,  5.2032e-01, -2.5719e-01, -1.8418e-01,  7.5692e-01,\n",
            "          9.9257e-01,  5.4539e-01,  2.4519e-01,  2.0212e+00, -3.2502e-01],\n",
            "        [-6.2030e-01,  1.1733e+00,  3.4114e-01, -6.2155e-01,  7.2907e-01,\n",
            "          2.3116e-01,  4.8400e-01, -9.3331e-02, -7.4156e-01, -3.1276e-01],\n",
            "        [-1.4881e+00, -1.1467e-01, -4.2221e-01, -4.2088e-02, -5.0256e-01,\n",
            "          4.5801e-01, -9.6180e-01, -1.9238e+00,  2.1504e+00,  7.4178e-02],\n",
            "        [ 3.5266e-01,  6.3841e-01,  2.4020e-01, -1.2913e+00,  1.9909e+00,\n",
            "         -1.1311e+00, -5.9834e-01, -1.5840e+00,  6.6748e-01,  3.7221e-01],\n",
            "        [ 1.0354e+00,  5.2108e-01,  1.0908e-01,  1.2988e+00,  1.2861e+00,\n",
            "         -1.1300e+00,  7.6871e-01, -2.5817e-01,  4.0185e-02,  2.1411e-01],\n",
            "        [-5.6694e-01, -6.6184e-01,  7.6766e-01, -1.1660e+00,  3.3846e-01,\n",
            "          1.7070e+00, -7.8003e-02,  6.9634e-02,  2.4800e-01,  1.3039e+00],\n",
            "        [-5.7571e-01,  5.6215e-01, -1.8286e-03, -1.1522e+00, -2.7140e+00,\n",
            "         -2.3121e-01, -2.6068e-01,  3.4016e-02, -8.6747e-01, -3.7597e-01],\n",
            "        [ 7.9598e-01,  7.7636e-01, -8.9482e-01,  1.0464e+00,  3.1344e-01,\n",
            "          4.8148e-01,  2.9755e-01, -5.0014e-01, -1.0113e+00,  1.2433e+00],\n",
            "        [-3.9263e-02,  4.7570e-01,  1.2599e-01,  2.8437e-01, -1.4078e+00,\n",
            "          1.1394e+00, -6.1797e-01, -9.9019e-01, -9.3729e-03,  8.3019e-01],\n",
            "        [ 4.4071e-01, -8.5637e-01, -3.4768e+00, -2.0894e-01,  7.1264e-01,\n",
            "         -7.7082e-01, -3.3618e-01,  5.9889e-01, -1.0815e-01,  8.5217e-01],\n",
            "        [ 1.2782e+00, -1.4924e+00,  7.7962e-01, -6.6727e-01,  6.3321e-01,\n",
            "          4.3855e-01, -5.7204e-01,  6.9065e-01,  9.3945e-02,  1.4197e-02],\n",
            "        [ 1.7273e+00, -1.1393e+00,  6.2623e-01,  9.7090e-01, -1.5068e+00,\n",
            "          1.1589e+00,  8.5678e-01,  1.2457e-01, -7.4142e-01,  1.6693e+00],\n",
            "        [-1.3728e+00,  8.4222e-01,  1.9095e+00, -1.1755e-01,  4.3974e-01,\n",
            "          1.0479e+00, -8.5650e-02, -8.9693e-01,  3.7739e-02,  1.8508e+00],\n",
            "        [ 6.7419e-01, -9.4675e-01,  7.8573e-02,  1.7481e+00,  6.4383e-01,\n",
            "          1.1807e-01,  1.3200e-01, -1.1332e+00, -1.1728e+00,  5.2352e-01],\n",
            "        [-4.3381e-01,  7.0410e-01, -1.7600e-01,  9.5339e-01, -1.1422e-02,\n",
            "          7.5449e-01, -1.6223e+00,  1.4349e+00,  1.6764e+00,  2.1302e-01],\n",
            "        [ 1.4578e+00, -3.6841e-01,  5.0739e-01, -1.9278e+00,  1.3475e+00,\n",
            "          4.2257e-01,  1.5130e+00,  1.4386e+00,  3.6167e-01,  2.4255e-01],\n",
            "        [-8.9288e-01,  1.3994e+00,  1.7622e-01,  1.8166e+00,  6.3786e-01,\n",
            "          1.0347e-01,  2.5772e+00, -9.0332e-02,  3.9320e-01, -9.3170e-01],\n",
            "        [ 1.0009e-01,  6.2441e-01,  3.7050e-01,  1.1594e-01,  4.5106e-01,\n",
            "         -1.3571e+00, -2.1217e+00, -1.8870e+00,  1.5370e+00,  7.2990e-02],\n",
            "        [ 5.9181e-01, -4.6409e-01,  1.3934e-01,  8.3414e-01, -1.1510e+00,\n",
            "          9.6631e-01,  2.7701e-01,  1.4577e+00, -1.3875e+00,  1.9749e+00],\n",
            "        [ 4.6483e-02, -6.7805e-01,  3.3211e-01,  3.0530e-01,  1.1114e+00,\n",
            "          2.1910e+00,  1.6084e-01, -8.0775e-01, -6.0614e-01, -1.3202e-01],\n",
            "        [-6.0700e-02, -1.1859e+00,  4.3848e-01,  9.4562e-01,  1.1985e+00,\n",
            "         -1.1362e-01, -1.0645e-01, -9.8950e-02,  1.3681e+00,  1.1089e+00],\n",
            "        [ 2.2907e-01, -8.2973e-01,  3.6108e-01,  1.4621e+00, -4.5836e-01,\n",
            "         -2.1903e+00, -4.2221e-01, -1.7965e+00,  3.5857e-01, -1.1103e+00],\n",
            "        [ 8.3633e-02,  1.0371e+00,  9.4864e-01, -3.2046e-01,  8.9689e-01,\n",
            "         -2.3954e+00, -6.3325e-01,  6.0317e-01, -1.1360e+00,  1.5507e+00],\n",
            "        [ 6.5291e-02, -3.3605e-01,  2.3866e-01, -1.6630e-01,  1.0634e+00,\n",
            "          3.1688e-01, -5.6205e-01,  1.7240e+00,  1.0379e+00, -5.2586e-01],\n",
            "        [ 1.5115e+00, -1.9400e-02, -4.8499e-01, -2.0091e-01, -1.5345e+00,\n",
            "         -7.6338e-01, -2.7167e-02, -6.2449e-02,  2.2189e+00,  7.7954e-01],\n",
            "        [ 2.3259e+00,  4.8114e-02, -1.3508e+00,  7.6020e-01,  1.0842e+00,\n",
            "          1.0491e+00,  6.6199e-01,  4.8401e-02, -4.6607e-01,  1.0567e+00],\n",
            "        [ 1.0191e+00,  1.4765e+00, -2.7258e+00,  2.2201e-01,  3.6495e-01,\n",
            "          1.0086e+00, -6.7033e-01, -8.5775e-01, -1.3292e+00, -1.9445e+00],\n",
            "        [-1.1735e+00, -2.3349e-02, -9.7868e-01,  1.0887e-01,  3.8872e-01,\n",
            "          1.0926e-01,  6.4019e-01, -1.4256e+00, -3.0211e-01, -1.3886e+00],\n",
            "        [-7.0251e-02, -2.5695e-01,  1.2861e+00,  2.2636e-02, -2.4984e-01,\n",
            "         -1.5417e+00,  8.5667e-01,  3.8696e-01, -7.7224e-02, -4.6288e-02],\n",
            "        [-1.3675e+00,  1.4585e-02,  5.4213e-01, -1.1366e+00, -5.8854e-02,\n",
            "         -8.2668e-01,  1.0003e+00,  1.0134e-02, -3.2781e-01,  6.3789e-01],\n",
            "        [ 2.9052e+00,  2.7637e-01,  5.3396e-01, -6.7002e-01,  3.3148e-01,\n",
            "         -6.7073e-01,  1.8002e-01,  5.4659e-01, -2.6934e-01,  4.1702e-01],\n",
            "        [-6.7070e-01, -3.3888e-02, -2.4120e-01,  9.6197e-01, -2.1480e-01,\n",
            "          2.7348e+00,  1.1374e+00, -1.0335e+00, -6.1981e-01, -1.4499e+00],\n",
            "        [-1.1720e+00,  1.1841e+00, -4.9357e-01, -7.0853e-01, -4.9747e-02,\n",
            "         -1.4898e+00, -1.3701e+00, -4.4572e-01,  6.3953e-01,  4.9581e-01],\n",
            "        [ 3.3572e-01, -1.2156e+00, -1.2413e+00, -2.0799e-01,  7.0212e-01,\n",
            "         -2.8655e-01,  9.4204e-01,  3.1964e-01,  2.0273e+00,  9.5357e-01],\n",
            "        [ 4.4092e-01,  1.1039e+00, -1.1304e+00, -1.7031e-01,  4.6828e-01,\n",
            "          3.3368e-01, -2.5486e-01, -4.3035e-01, -1.4421e+00,  1.0124e+00],\n",
            "        [-3.3586e-01,  7.5343e-01, -4.6219e-02, -1.2875e+00,  6.8694e-01,\n",
            "          1.6201e-01, -4.0172e-01, -1.0921e+00, -1.9442e+00,  1.0329e-01],\n",
            "        [ 2.7587e+00, -1.8957e-01,  1.4873e+00,  7.8509e-01,  5.1546e-01,\n",
            "         -1.0346e+00, -1.3660e+00, -1.3105e+00, -1.2664e+00, -5.1266e-02],\n",
            "        [ 9.6912e-01,  1.5580e-01,  9.1932e-02, -9.4646e-01,  4.5146e-01,\n",
            "         -4.6888e-01,  9.9506e-01,  1.4703e-02, -4.9286e-01,  5.9387e-01],\n",
            "        [-8.0438e-01, -1.7381e+00,  2.1208e+00, -1.0027e+00, -1.7958e-01,\n",
            "          1.0480e+00,  2.5041e-01,  1.1160e+00, -1.0849e+00, -6.8470e-01],\n",
            "        [ 8.3641e-02,  1.4250e-01, -7.5208e-02, -1.2171e+00,  6.3551e-01,\n",
            "         -1.0103e+00, -1.1609e+00, -7.6038e-01, -1.8399e+00,  7.1159e-01],\n",
            "        [-2.6726e-01, -8.6606e-01, -8.2820e-01,  8.2191e-02,  2.0870e+00,\n",
            "          1.2798e+00,  1.3590e+00, -9.0719e-01, -3.0933e-01,  3.1029e-01],\n",
            "        [ 6.6289e-01,  4.6683e-01, -7.7787e-01,  3.1527e-01, -1.5125e-01,\n",
            "         -1.0342e+00, -1.4502e+00,  1.2635e-02,  3.6522e-02, -3.2554e-01],\n",
            "        [-4.8857e-01, -6.4775e-01, -9.0373e-01, -1.4166e+00,  1.3034e+00,\n",
            "         -7.5582e-01, -7.8056e-01, -7.5438e-01, -8.3174e-01,  3.4519e-01]])\n",
            "0 35017656.0\n",
            "1 36887400.0\n",
            "2 44789436.0\n",
            "3 48419156.0\n",
            "4 39386768.0\n",
            "5 21795180.0\n",
            "6 9000738.0\n",
            "7 3575981.0\n",
            "8 1801334.25\n",
            "9 1185583.5\n",
            "10 910360.375\n",
            "11 745790.75\n",
            "12 627789.8125\n",
            "13 535518.375\n",
            "14 460748.1875\n",
            "15 399129.4375\n",
            "16 347861.9375\n",
            "17 304728.09375\n",
            "18 268207.46875\n",
            "19 237088.28125\n",
            "20 210421.21875\n",
            "21 187424.921875\n",
            "22 167494.9375\n",
            "23 150162.34375\n",
            "24 135022.171875\n",
            "25 121737.21875\n",
            "26 110025.765625\n",
            "27 99674.71875\n",
            "28 90509.4296875\n",
            "29 82358.828125\n",
            "30 75086.6171875\n",
            "31 68582.9609375\n",
            "32 62754.30078125\n",
            "33 57518.453125\n",
            "34 52799.5234375\n",
            "35 48538.453125\n",
            "36 44682.90625\n",
            "37 41188.17578125\n",
            "38 38013.34375\n",
            "39 35124.94921875\n",
            "40 32493.21875\n",
            "41 30094.69921875\n",
            "42 27903.75390625\n",
            "43 25899.9375\n",
            "44 24064.958984375\n",
            "45 22382.279296875\n",
            "46 20836.81640625\n",
            "47 19414.541015625\n",
            "48 18103.63671875\n",
            "49 16892.541015625\n",
            "50 15774.5732421875\n",
            "51 14741.541015625\n",
            "52 13785.3154296875\n",
            "53 12899.841796875\n",
            "54 12078.771484375\n",
            "55 11317.228515625\n",
            "56 10609.6640625\n",
            "57 9951.775390625\n",
            "58 9340.0068359375\n",
            "59 8770.8837890625\n",
            "60 8240.56640625\n",
            "61 7745.99169921875\n",
            "62 7284.34375\n",
            "63 6853.38623046875\n",
            "64 6450.8154296875\n",
            "65 6074.662109375\n",
            "66 5722.837890625\n",
            "67 5393.4443359375\n",
            "68 5084.9638671875\n",
            "69 4796.01220703125\n",
            "70 4524.9921875\n",
            "71 4270.74755859375\n",
            "72 4032.323974609375\n",
            "73 3808.5185546875\n",
            "74 3598.326171875\n",
            "75 3400.737548828125\n",
            "76 3214.967041015625\n",
            "77 3040.23388671875\n",
            "78 2875.797607421875\n",
            "79 2721.041748046875\n",
            "80 2575.298095703125\n",
            "81 2437.9853515625\n",
            "82 2308.590087890625\n",
            "83 2186.55615234375\n",
            "84 2071.531494140625\n",
            "85 1962.9560546875\n",
            "86 1860.53759765625\n",
            "87 1763.8448486328125\n",
            "88 1672.562744140625\n",
            "89 1586.307861328125\n",
            "90 1504.794921875\n",
            "91 1427.7647705078125\n",
            "92 1354.965576171875\n",
            "93 1286.115478515625\n",
            "94 1221.00390625\n",
            "95 1159.376708984375\n",
            "96 1101.0645751953125\n",
            "97 1045.8802490234375\n",
            "98 993.6373291015625\n",
            "99 944.1370849609375\n",
            "100 897.2561645507812\n",
            "101 852.8309326171875\n",
            "102 810.7435302734375\n",
            "103 770.853271484375\n",
            "104 733.0318603515625\n",
            "105 697.1641845703125\n",
            "106 663.1507568359375\n",
            "107 630.9129638671875\n",
            "108 600.315185546875\n",
            "109 571.2770385742188\n",
            "110 543.7122802734375\n",
            "111 517.5555419921875\n",
            "112 492.7159423828125\n",
            "113 469.12908935546875\n",
            "114 446.72369384765625\n",
            "115 425.44891357421875\n",
            "116 405.2471923828125\n",
            "117 386.05511474609375\n",
            "118 367.80242919921875\n",
            "119 350.4583740234375\n",
            "120 333.9759826660156\n",
            "121 318.2963562011719\n",
            "122 303.38427734375\n",
            "123 289.20281982421875\n",
            "124 275.71087646484375\n",
            "125 262.88177490234375\n",
            "126 250.67176818847656\n",
            "127 239.0561065673828\n",
            "128 227.9964599609375\n",
            "129 217.4713897705078\n",
            "130 207.45333862304688\n",
            "131 197.9136199951172\n",
            "132 188.83717346191406\n",
            "133 180.19300842285156\n",
            "134 171.96054077148438\n",
            "135 164.11538696289062\n",
            "136 156.64346313476562\n",
            "137 149.52423095703125\n",
            "138 142.7416534423828\n",
            "139 136.27821350097656\n",
            "140 130.1173858642578\n",
            "141 124.24859619140625\n",
            "142 118.6530990600586\n",
            "143 113.32320404052734\n",
            "144 108.23778533935547\n",
            "145 103.38990783691406\n",
            "146 98.767578125\n",
            "147 94.35812377929688\n",
            "148 90.15204620361328\n",
            "149 86.13912963867188\n",
            "150 82.31109619140625\n",
            "151 78.66085815429688\n",
            "152 75.17713928222656\n",
            "153 71.85249328613281\n",
            "154 68.68023681640625\n",
            "155 65.65216064453125\n",
            "156 62.762351989746094\n",
            "157 60.00321960449219\n",
            "158 57.37042236328125\n",
            "159 54.858314514160156\n",
            "160 52.4582405090332\n",
            "161 50.16639709472656\n",
            "162 47.97589874267578\n",
            "163 45.88618469238281\n",
            "164 43.890316009521484\n",
            "165 41.98341369628906\n",
            "166 40.16173553466797\n",
            "167 38.421142578125\n",
            "168 36.75961685180664\n",
            "169 35.17104721069336\n",
            "170 33.653194427490234\n",
            "171 32.20402526855469\n",
            "172 30.818431854248047\n",
            "173 29.493488311767578\n",
            "174 28.227689743041992\n",
            "175 27.017915725708008\n",
            "176 25.862096786499023\n",
            "177 24.756235122680664\n",
            "178 23.698606491088867\n",
            "179 22.68815803527832\n",
            "180 21.721757888793945\n",
            "181 20.797409057617188\n",
            "182 19.91357421875\n",
            "183 19.06890869140625\n",
            "184 18.261201858520508\n",
            "185 17.488309860229492\n",
            "186 16.748472213745117\n",
            "187 16.04189109802246\n",
            "188 15.364619255065918\n",
            "189 14.717548370361328\n",
            "190 14.098562240600586\n",
            "191 13.5058012008667\n",
            "192 12.938796043395996\n",
            "193 12.396331787109375\n",
            "194 11.877225875854492\n",
            "195 11.380195617675781\n",
            "196 10.904685974121094\n",
            "197 10.449299812316895\n",
            "198 10.013686180114746\n",
            "199 9.597021102905273\n",
            "200 9.197358131408691\n",
            "201 8.815539360046387\n",
            "202 8.449607849121094\n",
            "203 8.099015235900879\n",
            "204 7.763545989990234\n",
            "205 7.442643165588379\n",
            "206 7.134530544281006\n",
            "207 6.840293884277344\n",
            "208 6.557863712310791\n",
            "209 6.287757873535156\n",
            "210 6.028841495513916\n",
            "211 5.780924320220947\n",
            "212 5.543583869934082\n",
            "213 5.316042423248291\n",
            "214 5.0981645584106445\n",
            "215 4.889408111572266\n",
            "216 4.689227104187012\n",
            "217 4.497743606567383\n",
            "218 4.313891887664795\n",
            "219 4.137975692749023\n",
            "220 3.9693448543548584\n",
            "221 3.8077876567840576\n",
            "222 3.6529645919799805\n",
            "223 3.504530906677246\n",
            "224 3.362248420715332\n",
            "225 3.2258496284484863\n",
            "226 3.0952818393707275\n",
            "227 2.9698286056518555\n",
            "228 2.8496649265289307\n",
            "229 2.734635353088379\n",
            "230 2.624081611633301\n",
            "231 2.5183959007263184\n",
            "232 2.41682767868042\n",
            "233 2.319523811340332\n",
            "234 2.226173162460327\n",
            "235 2.136935234069824\n",
            "236 2.0511186122894287\n",
            "237 1.9688293933868408\n",
            "238 1.8897912502288818\n",
            "239 1.8139578104019165\n",
            "240 1.741391658782959\n",
            "241 1.6718201637268066\n",
            "242 1.60512113571167\n",
            "243 1.5408401489257812\n",
            "244 1.4794749021530151\n",
            "245 1.4206032752990723\n",
            "246 1.3640996217727661\n",
            "247 1.3098031282424927\n",
            "248 1.2576587200164795\n",
            "249 1.2078286409378052\n",
            "250 1.1599256992340088\n",
            "251 1.113801121711731\n",
            "252 1.0696965456008911\n",
            "253 1.027317762374878\n",
            "254 0.9866458177566528\n",
            "255 0.9478024244308472\n",
            "256 0.910305380821228\n",
            "257 0.8743815422058105\n",
            "258 0.8398523926734924\n",
            "259 0.8067428469657898\n",
            "260 0.77498459815979\n",
            "261 0.7444092631340027\n",
            "262 0.7151080369949341\n",
            "263 0.6870681643486023\n",
            "264 0.6600555181503296\n",
            "265 0.6341674327850342\n",
            "266 0.6093223094940186\n",
            "267 0.5855035781860352\n",
            "268 0.5625856518745422\n",
            "269 0.5404824614524841\n",
            "270 0.5193469524383545\n",
            "271 0.4991593658924103\n",
            "272 0.4796674847602844\n",
            "273 0.4609643816947937\n",
            "274 0.4429834187030792\n",
            "275 0.42564237117767334\n",
            "276 0.40905332565307617\n",
            "277 0.3931596279144287\n",
            "278 0.3778756558895111\n",
            "279 0.3632045090198517\n",
            "280 0.34910663962364197\n",
            "281 0.33556127548217773\n",
            "282 0.3225591480731964\n",
            "283 0.31007662415504456\n",
            "284 0.29804423451423645\n",
            "285 0.2864460051059723\n",
            "286 0.27539828419685364\n",
            "287 0.2647494077682495\n",
            "288 0.25454163551330566\n",
            "289 0.24467815458774567\n",
            "290 0.23523956537246704\n",
            "291 0.2261783331632614\n",
            "292 0.2174689620733261\n",
            "293 0.20910578966140747\n",
            "294 0.20106428861618042\n",
            "295 0.19330041110515594\n",
            "296 0.1858149766921997\n",
            "297 0.17871831357479095\n",
            "298 0.1718578338623047\n",
            "299 0.16528545320034027\n",
            "300 0.15890544652938843\n",
            "301 0.15283973515033722\n",
            "302 0.14697887003421783\n",
            "303 0.1413278430700302\n",
            "304 0.13592106103897095\n",
            "305 0.13073815405368805\n",
            "306 0.12576629221439362\n",
            "307 0.12095094472169876\n",
            "308 0.11636289954185486\n",
            "309 0.11190058290958405\n",
            "310 0.10764475166797638\n",
            "311 0.10353406518697739\n",
            "312 0.09957845509052277\n",
            "313 0.09580224752426147\n",
            "314 0.09214196354150772\n",
            "315 0.0886404886841774\n",
            "316 0.08526631444692612\n",
            "317 0.08202419430017471\n",
            "318 0.07890824973583221\n",
            "319 0.07590316236019135\n",
            "320 0.07302380353212357\n",
            "321 0.07027004659175873\n",
            "322 0.06758849322795868\n",
            "323 0.0650409683585167\n",
            "324 0.06258779019117355\n",
            "325 0.06022457033395767\n",
            "326 0.057963281869888306\n",
            "327 0.0557800754904747\n",
            "328 0.05367671698331833\n",
            "329 0.051650166511535645\n",
            "330 0.0496966689825058\n",
            "331 0.047821130603551865\n",
            "332 0.046029116958379745\n",
            "333 0.044303447008132935\n",
            "334 0.04262147098779678\n",
            "335 0.041036248207092285\n",
            "336 0.03950003534555435\n",
            "337 0.038024790585041046\n",
            "338 0.036593254655599594\n",
            "339 0.03521081805229187\n",
            "340 0.033906225115060806\n",
            "341 0.03262554481625557\n",
            "342 0.03140753135085106\n",
            "343 0.03023344837129116\n",
            "344 0.029116321355104446\n",
            "345 0.02802964299917221\n",
            "346 0.0269756019115448\n",
            "347 0.025989403948187828\n",
            "348 0.025014130398631096\n",
            "349 0.024078741669654846\n",
            "350 0.023193297907710075\n",
            "351 0.022333180531859398\n",
            "352 0.021506693214178085\n",
            "353 0.020707163959741592\n",
            "354 0.019948560744524002\n",
            "355 0.01921479031443596\n",
            "356 0.0185075830668211\n",
            "357 0.01781473495066166\n",
            "358 0.01715938001871109\n",
            "359 0.01653902605175972\n",
            "360 0.01593783125281334\n",
            "361 0.015347212553024292\n",
            "362 0.014787532389163971\n",
            "363 0.014247335493564606\n",
            "364 0.013728085905313492\n",
            "365 0.013225150294601917\n",
            "366 0.012747567147016525\n",
            "367 0.012279888615012169\n",
            "368 0.011837175115942955\n",
            "369 0.011411997489631176\n",
            "370 0.011000319384038448\n",
            "371 0.010601656511425972\n",
            "372 0.010227572172880173\n",
            "373 0.009859342128038406\n",
            "374 0.009505595080554485\n",
            "375 0.009168023243546486\n",
            "376 0.008839743211865425\n",
            "377 0.008519022725522518\n",
            "378 0.008212113752961159\n",
            "379 0.007922514341771603\n",
            "380 0.007638905197381973\n",
            "381 0.007371598854660988\n",
            "382 0.0071122897788882256\n",
            "383 0.00686107762157917\n",
            "384 0.006624747067689896\n",
            "385 0.006391626782715321\n",
            "386 0.00616217078641057\n",
            "387 0.005950313061475754\n",
            "388 0.005748834926635027\n",
            "389 0.005547232925891876\n",
            "390 0.005354051943868399\n",
            "391 0.005173089914023876\n",
            "392 0.004998126532882452\n",
            "393 0.004824399948120117\n",
            "394 0.004661169834434986\n",
            "395 0.0045006414875388145\n",
            "396 0.0043510752730071545\n",
            "397 0.004204044118523598\n",
            "398 0.00406539486721158\n",
            "399 0.003930750302970409\n",
            "400 0.003797688987106085\n",
            "401 0.0036709147971123457\n",
            "402 0.0035515869967639446\n",
            "403 0.003437089268118143\n",
            "404 0.003321937518194318\n",
            "405 0.0032093687914311886\n",
            "406 0.003106754506006837\n",
            "407 0.003004266880452633\n",
            "408 0.0029105471912771463\n",
            "409 0.002815446350723505\n",
            "410 0.00272498931735754\n",
            "411 0.002637523924931884\n",
            "412 0.0025525959208607674\n",
            "413 0.0024715035688132048\n",
            "414 0.002394038252532482\n",
            "415 0.002318757586181164\n",
            "416 0.002246032003313303\n",
            "417 0.002176194917410612\n",
            "418 0.002109209541231394\n",
            "419 0.002046093577519059\n",
            "420 0.0019831156823784113\n",
            "421 0.001922228722833097\n",
            "422 0.001864238060079515\n",
            "423 0.0018065719632431865\n",
            "424 0.0017529246397316456\n",
            "425 0.0017010732553899288\n",
            "426 0.0016500171041116118\n",
            "427 0.0016006708610802889\n",
            "428 0.0015515524428337812\n",
            "429 0.0015042638406157494\n",
            "430 0.0014589970232918859\n",
            "431 0.0014176463009789586\n",
            "432 0.0013750526122748852\n",
            "433 0.0013343701139092445\n",
            "434 0.001295831403695047\n",
            "435 0.0012594072613865137\n",
            "436 0.0012245549587532878\n",
            "437 0.0011892912443727255\n",
            "438 0.0011541058775037527\n",
            "439 0.001122429035604\n",
            "440 0.0010915957391262054\n",
            "441 0.0010599582456052303\n",
            "442 0.0010312995873391628\n",
            "443 0.0010027863318100572\n",
            "444 0.0009758640080690384\n",
            "445 0.0009497193968854845\n",
            "446 0.0009246841073036194\n",
            "447 0.0008991588838398457\n",
            "448 0.0008753908914513886\n",
            "449 0.0008519997354596853\n",
            "450 0.0008298174943774939\n",
            "451 0.0008070700569078326\n",
            "452 0.000786534626968205\n",
            "453 0.000765063741710037\n",
            "454 0.0007453526486642659\n",
            "455 0.0007272859802469611\n",
            "456 0.0007089718128554523\n",
            "457 0.0006911115488037467\n",
            "458 0.0006734057678841054\n",
            "459 0.0006567914970219135\n",
            "460 0.000639747828245163\n",
            "461 0.0006239242502488196\n",
            "462 0.0006084130727685988\n",
            "463 0.0005939377006143332\n",
            "464 0.0005791533039882779\n",
            "465 0.0005650228704325855\n",
            "466 0.0005511475610546768\n",
            "467 0.0005384745309129357\n",
            "468 0.0005267012165859342\n",
            "469 0.0005137283005751669\n",
            "470 0.0005012061446905136\n",
            "471 0.0004893989535048604\n",
            "472 0.000478335190564394\n",
            "473 0.00046826875768601894\n",
            "474 0.00045691474224440753\n",
            "475 0.00044616975355893373\n",
            "476 0.00043676927452906966\n",
            "477 0.0004261682915966958\n",
            "478 0.0004167096922174096\n",
            "479 0.0004072983574587852\n",
            "480 0.00039867209852673113\n",
            "481 0.00038994327769614756\n",
            "482 0.0003812489740084857\n",
            "483 0.00037271028850227594\n",
            "484 0.00036437821108847857\n",
            "485 0.00035636755637824535\n",
            "486 0.00034955079900100827\n",
            "487 0.00034214096376672387\n",
            "488 0.0003347062738612294\n",
            "489 0.0003275543567724526\n",
            "490 0.000320703285979107\n",
            "491 0.00031410495284944773\n",
            "492 0.00030714337481185794\n",
            "493 0.00030152854742482305\n",
            "494 0.000295847887173295\n",
            "495 0.00028903872589580715\n",
            "496 0.0002840990200638771\n",
            "497 0.00027824376593343914\n",
            "498 0.0002725207305047661\n",
            "499 0.00026661207084544003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycwxAPHZLNST"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Question 12\n",
        "\n",
        "In the code above, why do we have 2 in '2.0*(y_pred - y)`?\n",
        "\n",
        "## Question 13\n",
        "In the code above, what does `grad_h[h < 0] = 0` signify?\n",
        "\n",
        "## Question 14\n",
        "In the code above, how many \"epochs\" have we trained the model for? \n",
        "\n",
        "## Question 15\n",
        "In the code above, if we take the trained model, and run it on fresh  inputs, the trained model will be able to predict fresh output with high accuracy. \n",
        "\n",
        "## Question 16\n",
        "In the code above, if we dont use clone in `grad_h = grad_h_relu.clone()` the model will still train without any issues. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xggxLfHjOIKj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrIKywlhOQZD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ6Cxw0CAIsS"
      },
      "source": [
        "Question 12\n",
        "because we have calculated the gradient "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyRKT8cnORdl"
      },
      "source": [
        "Question 13 Relu operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_M7fraqW0Mi"
      },
      "source": [
        "Question 14 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFzMFMiuYM_8"
      },
      "source": [
        "Question 15 False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHSwbXWZDcd"
      },
      "source": [
        "Question 16 Yes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnJ49I07JJI6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE2zeXsYWzHp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HS304WQOL9L"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FMSk_MuOLJk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}